# Project Hunter

**Autonomous Intelligence System for Google Discover**

Discovers the "secret sauce" of Google Discover by monitoring 100+ competitors, extracting article DNA, and identifying winning patterns.

---

## ğŸ¯ What It Does

1. **Discovers** 100+ competitors from 7 seed URLs
2. **Monitors** RSS feeds every 60 seconds
3. **Extracts** 20+ DNA data points per article
4. **Analyzes** patterns to identify what works
5. **Reports** winning niche + actionable blueprint

---

## ğŸ“¦ Installation & Setup

### **Prerequisites**

- Python 3.11 or higher
- Git
- API keys (Anthropic Claude, OpenAI optional)

---

### **Setup for Mac**

```bash
# 1. Clone or navigate to project
cd /path/to/project-hunter

# 2. Create virtual environment
python3 -m venv venv

# 3. Activate virtual environment
source venv/bin/activate

# 4. Install Python dependencies
pip install -r requirements.txt

# 5. Install Playwright browsers
playwright install chromium

# 6. Create .env file (see API Keys section below)
touch .env
```

---

### **Setup for Windows**

```cmd
# 1. Clone or navigate to project
cd C:\path\to\project-hunter

# 2. Create virtual environment
python -m venv venv

# 3. Activate virtual environment
venv\Scripts\activate

# 4. Install Python dependencies
pip install -r requirements.txt

# 5. Install Playwright browsers
playwright install chromium

# 6. Create .env file (see API Keys section below)
# Create file manually or use: echo. > .env
```

---

### **API Keys Configuration**

Create a `.env` file in the project root:

```env
# Required
ANTHROPIC_API_KEY=sk-ant-api03-xxxxx

# Optional (for fallback/cross-verification)
OPENAI_API_KEY=sk-proj-xxxxx

# Optional (for social velocity tracking - future feature)
NETROWS_API_KEY=pk_live_xxxxx
```

**Where to get API keys:**
- **Anthropic Claude:** https://console.anthropic.com/
- **OpenAI:** https://platform.openai.com/api-keys
- **Netrows:** https://netrows.com/ (optional)

---

### **Verify Installation**

**Mac/Linux:**
```bash
python scripts/run_discovery.py --help
```

**Windows:**
```cmd
python scripts\run_discovery.py --help
```

If you see help text, you're good to go!

---

## ğŸ“Š NEW: Real-Time Dashboard

**Visualize everything in your browser!**

```bash
streamlit run dashboard.py
```

**Features:**
- ğŸ“Š Real-time domain capture visualization
- ğŸ” Interactive competitor browser with filters
- ğŸ“ˆ Intelligence analysis with charts
- ğŸ“¥ CSV export functionality
- âš™ï¸ System status monitoring

Opens automatically at: `http://localhost:8501`

See: [DASHBOARD_GUIDE.md](DASHBOARD_GUIDE.md) for full details

---

## ğŸš€ Quick Start

### **NEW: Chrome Extension Method (Recommended)**

**Uses passive monitoring - 100% legal, zero ban risk, most accurate**

1. **Install Chrome Extension** (see [SETUP_GUIDE.md](SETUP_GUIDE.md) for details):
   ```bash
   # Open chrome://extensions/
   # Enable Developer Mode
   # Load unpacked -> select chrome_extension/ folder
   ```

2. **Start API Server**:
   ```bash
   python api/discover_api.py
   ```

3. **Browse Google Discover** (30-60 minutes casual browsing):
   - Go to google.com or google.com/discover
   - Scroll through your feed naturally
   - Extension captures domains automatically
   - Target: 100-200 domains

4. **Load Discovered Competitors**:
   ```bash
   python scripts/run_discovery.py
   ```

**Why this method?**
- âœ… Discovers sites ACTUALLY in Google Discover (not random blogs)
- âœ… Zero ban risk (no automation, just observing your own feed)
- âœ… 100% legal (your own browsing data)
- âœ… Most accurate results

See full setup guide: [SETUP_GUIDE.md](SETUP_GUIDE.md)

---

### **Alternative: BFS Crawler Method** (Original)

Discovers competitors by crawling from seed URLs.

**Mac/Linux:**
```bash
python scripts/run_discovery.py
```

**Windows:**
```cmd
python scripts\run_discovery.py
```

**What happens:**
- Crawls 7 seed URLs from `config/seed_urls.yaml`
- Discovers 100-150 competitor sites using BFS algorithm
- Auto-detects RSS feeds for all competitors
- Saves to `data/competitors/`

**Note:** This finds sites that link to each other, but doesn't guarantee they're in Google Discover.

---

### **Step 2: Start Monitoring** (24-48 hours recommended)

Monitors RSS feeds and extracts article DNA.

**Mac/Linux:**
```bash
# Test run (5 cycles)
python scripts/run_monitor.py --cycles 5

# Production (runs indefinitely)
python scripts/run_monitor.py
```

**Windows:**
```cmd
# Test run (5 cycles)
python scripts\run_monitor.py --cycles 5

# Production (runs indefinitely)
python scripts\run_monitor.py
```

**What happens:**
- Monitors 100+ RSS feeds every 60 seconds
- Detects new articles (GUID-based detection)
- Extracts DNA profiles (20+ data points per article)
- Runs intelligence analysis every 6 hours
- Saves to `data/articles/articles.db`

**Tip:** Let it run for 24-48 hours to collect sufficient data (500+ articles)

---

### **Step 3: Generate Intelligence Report**

Analyzes all data and identifies winning patterns.

**Mac/Linux:**
```bash
python scripts/generate_report.py
```

**Windows:**
```cmd
python scripts\generate_report.py
```

**What you get:**
- Niche velocity scores (0-100)
- Winning niche recommendation
- Structural blueprint (word count, images, schema)
- Title formulas (LLM-extracted patterns)
- Timing strategy (optimal publish windows)

---

## ğŸ“Š What You Get

âœ… **Winning Niche** - Score 0-100, clear recommendation (HOT/WARM/MODERATE/COLD)
âœ… **Structural Blueprint** - Word count, images, schema, HTML structure
âœ… **Title Formulas** - LLM-extracted patterns that work
âœ… **Timing Strategy** - When to publish for max velocity
âœ… **Competitor Benchmarks** - Who's winning and why

---

## ğŸ“ˆ Expected Results (24-48 hours)

```
âœ“ Competitors: 100-150 sites
âœ“ RSS Feeds: 100-110 active
âœ“ Articles: 500-1000+
âœ“ DNA Profiles: 500+ complete (20+ data points each)
âœ“ Patterns: 6-10 structural patterns identified
âœ“ Winning Niche: Identified with 85+ confidence score
âœ“ Title Formulas: 10-20 proven patterns
âœ“ Timing Insights: 3+ optimal publish windows
```

**Cost:** $2-5 in API calls (LLM title analysis only)

---

## ğŸ“ Project Structure

```
project-hunter/
â”œâ”€â”€ chrome_extension/          # NEW: Chrome extension for passive monitoring
â”‚   â”œâ”€â”€ manifest.json
â”‚   â”œâ”€â”€ content.js
â”‚   â”œâ”€â”€ background.js
â”‚   â”œâ”€â”€ popup.html
â”‚   â””â”€â”€ popup.js
â”‚
â”œâ”€â”€ api/                       # NEW: FastAPI server
â”‚   â”œâ”€â”€ discover_api.py
â”‚   â””â”€â”€ test_api.py
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ seed_urls.yaml        # 7 seed competitors + discovery settings
â”‚   â””â”€â”€ niches.yaml            # 6 niches with keywords
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ scout/                 # Discovery & Monitoring
â”‚   â”‚   â”œâ”€â”€ competitor_discovery.py
â”‚   â”‚   â”œâ”€â”€ rss_discovery.py
â”‚   â”‚   â””â”€â”€ rss_monitor.py
â”‚   â”œâ”€â”€ architect/             # DNA Extraction
â”‚   â”‚   â””â”€â”€ dna_extractor.py
â”‚   â”œâ”€â”€ intelligence/          # Pattern Recognition
â”‚   â”‚   â”œâ”€â”€ pattern_engine.py
â”‚   â”‚   â”œâ”€â”€ niche_scorer.py
â”‚   â”‚   â”œâ”€â”€ title_analyzer.py
â”‚   â”‚   â””â”€â”€ timing_analyzer.py
â”‚   â”œâ”€â”€ orchestrator/          # Coordination
â”‚   â”‚   â”œâ”€â”€ main_controller.py
â”‚   â”‚   â”œâ”€â”€ task_queue.py
â”‚   â”‚   â””â”€â”€ rate_limiter.py
â”‚   â””â”€â”€ persistence/           # Data Layer
â”‚       â”œâ”€â”€ database.py
â”‚       â””â”€â”€ models.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_discovery.py      # Step 1
â”‚   â”œâ”€â”€ run_monitor.py         # Step 2
â”‚   â””â”€â”€ generate_report.py    # Step 3
â”‚
â”œâ”€â”€ data/                      # Auto-created during runtime
â”‚   â”œâ”€â”€ competitors/
â”‚   â”‚   â”œâ”€â”€ discovered_sites.json
â”‚   â”‚   â””â”€â”€ rss_feeds.json
â”‚   â”œâ”€â”€ articles/
â”‚   â”‚   â”œâ”€â”€ articles.db       # SQLite database
â”‚   â”‚   â””â”€â”€ dna_profiles/
â”‚   â””â”€â”€ intelligence/
â”‚       â”œâ”€â”€ patterns.json
â”‚       â”œâ”€â”€ niche_scores.json
â”‚       â”œâ”€â”€ title_formulas.json
â”‚       â””â”€â”€ timing_insights.json
â”‚
â”œâ”€â”€ .env                       # API keys (create this)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ SETUP_GUIDE.md            # NEW: Detailed Chrome extension setup
```

---

## ğŸ—ï¸ Architecture

```
Phase 1: Discovery (2-4 hours)
  â”œâ”€ BFS competitor crawler (7 seeds â†’ 100+ sites)
  â”œâ”€ RSS feed auto-detection
  â””â”€ Relevance scoring & filtering

Phase 2: Monitoring (continuous, 60s cycles)
  â”œâ”€ Async RSS polling (batches of 20)
  â”œâ”€ New article detection (GUID-based)
  â”œâ”€ DNA extraction queue (10 concurrent workers)
  â””â”€ Intelligence analysis (every 6 hours)

Phase 3: Intelligence (on-demand)
  â”œâ”€ Pattern aggregation (word count, images, schema, structure)
  â”œâ”€ Niche velocity scoring (0-100)
  â”œâ”€ LLM title analysis (Claude/GPT)
  â””â”€ Timing pattern analysis
```

**Tech Stack:**
- Python 3.11+ with asyncio
- Playwright (stealth mode for DNA extraction)
- feedparser (RSS parsing)
- aiohttp (async HTTP)
- SQLite (articles + DNA)
- Claude Sonnet 4.5 (title analysis)

---

## ğŸ’¡ Key Features

- âœ… **BFS Competitor Discovery** - Relevance scoring, depth-limited crawl
- âœ… **Async RSS Monitoring** - Batches of 20 feeds, 60-second cycles
- âœ… **DNA Extraction** - 20+ data points (title, word count, images, schema, structure)
- âœ… **LLM Title Analysis** - Extract proven patterns from 500+ titles
- âœ… **Niche Velocity Scoring** - Article volume (20%) + social (25%) + timing (15%) + patterns (40%)
- âœ… **Anti-Ban Protection** - Playwright stealth mode + rate limiting + random delays
- âœ… **Production Ready** - Error handling, retry logic, state management

---

## ğŸ”§ Configuration

### **Customize Seed URLs**

Edit `config/seed_urls.yaml`:
```yaml
seeds:
  - url: "https://your-competitor.com/"
    niche: "your-niche"

discovery:
  max_depth: 3              # Crawl depth (1-5)
  target_count: 150         # Target competitors
  min_relevance_score: 0.6  # Quality threshold
```

### **Customize Niches**

Edit `config/niches.yaml`:
```yaml
niches:
  your_niche:
    keywords: ["keyword1", "keyword2", "keyword3"]
    weight: 1.0
```

### **Adjust Performance**

**Monitoring speed** (edit `core/scout/rss_monitor.py`):
```python
RSSMonitor(
    batch_size=20,       # Feeds per batch (increase for faster)
    cycle_interval=60    # Seconds between cycles
)
```

**DNA workers** (edit `core/orchestrator/task_queue.py`):
```python
DNAExtractionQueue(max_workers=10)  # Concurrent extractions (5-20)
```

---

## ğŸ› Troubleshooting

### **"ModuleNotFoundError: No module named 'feedparser'"**
```bash
pip install -r requirements.txt
```

### **Playwright errors**
```bash
playwright install chromium
```

### **"No competitors found"**
- Check `config/seed_urls.yaml` has valid URLs
- Verify internet connection
- Check logs in `data/logs/monitoring.log`

### **"API key not found"**
- Ensure `.env` file exists in project root
- Check API key format (starts with `sk-ant-` for Anthropic)

### **Rate limit errors**
- Built-in rate limiting handles this automatically
- System will retry with exponential backoff

---

## ğŸ“Š Monitoring Progress

### **View Real-Time Logs**

**Mac/Linux:**
```bash
tail -f data/logs/monitoring.log
```

**Windows:**
```cmd
powershell Get-Content data\logs\monitoring.log -Wait -Tail 50
```

### **Check Database Stats**

**Mac/Linux:**
```bash
sqlite3 data/articles/articles.db "SELECT COUNT(*) FROM articles;"
```

**Windows:**
```cmd
sqlite3 data\articles\articles.db "SELECT COUNT(*) FROM articles;"
```

### **View Discovered Competitors**

**Mac/Linux:**
```bash
cat data/competitors/discovered_sites.json | python -m json.tool
```

**Windows:**
```cmd
type data\competitors\discovered_sites.json
```

---

## âš™ï¸ Requirements

**Minimum:**
- Python 3.11+
- 4GB RAM
- 2GB disk space
- Internet connection

**APIs (get free tier):**
- Anthropic Claude API (required for title analysis)
- OpenAI API (optional, for cross-verification)
- Netrows API (optional, for social velocity tracking)

---

## ğŸ¯ Use Cases

**1. Niche Research** - Discover which niche has highest velocity
**2. Content Strategy** - Copy winning structural patterns
**3. SEO Optimization** - Use proven title formulas
**4. Timing Strategy** - Publish during optimal windows
**5. Competitive Analysis** - Track what competitors are doing

---

## ğŸ“ Notes

- **First run takes time:** Discovery phase may take 2-4 hours (crawling 100+ sites)
- **Let it run:** System needs 24-48 hours to collect sufficient data (500+ articles)
- **API costs:** Only title analysis uses LLM (~$1-2 per 500 titles), rest is free
- **Deterministic output:** No - output changes with real-time data (this is good!)
- **Patterns stabilize:** After 1000+ articles, core patterns become consistent

---

## ğŸ“„ License

Proprietary - Project Hunter

---

## ğŸš€ Next Steps After Intelligence Report

1. âœ… **Review winning niche** - Focus your content efforts
2. âœ… **Study structural blueprint** - Word count, images, schema requirements
3. âœ… **Apply title formulas** - Use proven headline patterns
4. âœ… **Follow timing strategy** - Publish during optimal windows
5. âœ… **Acquire domain** - Buy aged domain in winning niche
6. âœ… **Create content** - Use the blueprint to dominate Google Discover!

---

**Status:** Production Ready

**Get started:** `python scripts/run_discovery.py` ğŸš€
